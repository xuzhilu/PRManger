"""
ä»£ç åˆ†ææ™ºèƒ½ä½“èŠ‚ç‚¹
ç›´æ¥åˆ†ædiffä»£ç å—ï¼Œåˆ¤æ–­ä¿®æ”¹å¯¹å·¥ç¨‹çš„å½±å“
æ”¯æŒè¿­ä»£å¼ä¸Šä¸‹æ–‡æ”¶é›†
"""

from langgraph.config import get_stream_writer
from src.core.state import PRReviewState
from langchain_core.messages import SystemMessage, HumanMessage
from src.utils.llm import llm, parser
from src.analyzers.project_analyzer import ASTParser, AST_AVAILABLE
from src.utils.config import CONFIG
import json
import os
from typing import Dict, List, Any


async def code_analyzer_node(state: PRReviewState) -> PRReviewState:
    """ä»£ç åˆ†ææ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    1. ä½¿ç”¨å¤§æ¨¡å‹åˆ†ædiffä»£ç å—
    2. åˆ¤æ–­ä¿®æ”¹æ˜¯å¦ä¼šå¯¹å·¥ç¨‹äº§ç”Ÿå½±å“
    3. å¦‚éœ€ä¸Šä¸‹æ–‡ï¼Œå‘ä¸Šä¸‹æ–‡æ”¶é›†æ™ºèƒ½ä½“è¯·æ±‚
    4. æ ¹æ®diffå’Œä¸Šä¸‹æ–‡ç»§ç»­åˆ†æ
    5. ç›´åˆ°å¾—å‡ºæ˜ç¡®ç»“è®ºæˆ–è¾¾åˆ°è¿­ä»£ä¸Šé™
    """
    print("\n" + "="*60)
    print("=== ä»£ç åˆ†ææ™ºèƒ½ä½“ ===")
    print("="*60)
    writer = get_stream_writer()
    
    # è·å–åŸºæœ¬ä¿¡æ¯
    pr_diff = state.get("pr_diff", "")
    changed_files = state.get("changed_files", [])
    pr_size = state.get("pr_size", "medium")
    
    # è¿­ä»£å¼åˆ†æç›¸å…³çŠ¶æ€
    iteration_count = state.get("iteration_count", 0)
    impact_chain = state.get("impact_chain", [])
    
    # ASTè§£æï¼šåªåœ¨é¦–è½®è§£æchanged_files
    repo_path = CONFIG['git_repo']['repo_path']
    ast_cache = state.get("ast_cache", {})
    updated_ast_cache = ast_cache
    
    if iteration_count == 0:
        # é¦–è½®ï¼šè§£æchanged_filesçš„ASTå¹¶å­˜å…¥ç¼“å­˜-----astæç¤ºè¯ï¼Œastç»“æ„
        ast_context, updated_ast_cache = _extract_ast_context(changed_files, repo_path, pr_size, ast_cache)
    else:
        # åç»­è½®ï¼šç›´æ¥ä½¿ç”¨ç¼“å­˜ä¸­çš„AST
        ast_context = _generate_ast_context_from_cache(changed_files, ast_cache, pr_size)
    
    # æ£€æŸ¥ä¸Šä¸‹æ–‡å“åº”
    context_response = state.get("context_response")
    
    print(f"[ä»£ç åˆ†æ] ğŸ“Š å½“å‰çŠ¶æ€: è¿­ä»£ {iteration_count + 1}")
    
    # ä»é…ç½®è·å–æ·±åº¦åˆ†æå‚æ•°
    deep_analysis_config = CONFIG.get('pr_review', {}).get('deep_analysis', {})
    max_iterations = deep_analysis_config.get('max_iterations', 6)
    
    # æ ¹æ®è§„æ¨¡æ™ºèƒ½è°ƒæ•´å‚æ•°
    size_config = {
        'small': {
            'max_iterations': max_iterations,
            'diff_chars': 6000,
            'max_defs_per_request': 10,
            'max_files': 10,
            'context_summary_chars': 9000
        },
        'medium': {
            'max_iterations': max_iterations,
            'diff_chars': 4500,
            'max_defs_per_request': 5,
            'max_files': 5,
            'context_summary_chars': 6000
        },
        'large': {
            'max_iterations': max_iterations - 2 if max_iterations > 2 else 2,
            'diff_chars': 3000,
            'max_defs_per_request': 3,
            'max_files': 3,
            'context_summary_chars': 4500
        },
        'xlarge': {
            'max_iterations': max_iterations - 4 if max_iterations > 4 else 2,
            'diff_chars': 2400,
            'max_defs_per_request': 2,
            'max_files': 2,
            'context_summary_chars': 3000
        }
    }
    
    config = size_config.get(pr_size, size_config['medium'])
    max_iterations = config['max_iterations']
    
    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    if iteration_count >= max_iterations:
        print(f"[ä»£ç åˆ†æ] âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})ï¼Œå¼ºåˆ¶å®Œæˆåˆ†æ")
        return {
            "analysis_conclusion": {
                "has_critical_issues": False,
                "critical_issues": [],
                "potential_risks": [f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ·±åº¦({max_iterations})ï¼Œå¯èƒ½å­˜åœ¨æœªå‘ç°çš„æ·±å±‚å½±å“"],
                "summary": f"å®Œæˆ {iteration_count} è½®è¿­ä»£åˆ†æ",
                "iteration_info": {
                    "total_iterations": iteration_count,
                    "impact_chain_depth": len(impact_chain)
                }
            },
            "current_stage": "analysis_complete",
            "iteration_count": iteration_count
        }
    
    # æ„å»ºsystem prompt - ç›´æ¥åˆ†ædiffä»£ç å—
    system_prompt = """ä½ æ˜¯ä»£ç å½±å“åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†ædiffä»£ç å—ï¼Œåˆ¤æ–­ä¿®æ”¹æ˜¯å¦ä¼šå¯¹å·¥ç¨‹äº§ç”Ÿå½±å“ã€‚

                    ## åˆ†ææµç¨‹
                    1. **é¦–è½®**ï¼šç›´æ¥åˆ†ædiffä»£ç å—
                    - ç†è§£ä»£ç ä¿®æ”¹çš„å…·ä½“å†…å®¹ï¼ˆæ–°å¢/ä¿®æ”¹/åˆ é™¤ï¼‰
                    - åˆ¤æ–­è¿™äº›ä¿®æ”¹æ˜¯å¦å¯èƒ½å½±å“å…¶ä»–æ¨¡å—
                    - å¦‚éœ€è¦äº†è§£æŸä¸ªå‡½æ•°/ç±»çš„ä½¿ç”¨æƒ…å†µï¼Œè¯·æ±‚ä¸Šä¸‹æ–‡æ”¶é›†

                    2. **åç»­è½®**ï¼šç»“åˆä¸Šä¸‹æ–‡ç»§ç»­åˆ†æ
                    - åˆ†ææ”¶é›†åˆ°çš„ä»£ç ç‰‡æ®µï¼Œç†è§£å®é™…è°ƒç”¨å…³ç³»
                    - åˆ¤æ–­è¿™äº›è°ƒç”¨æ˜¯å¦ä¼šå—åˆ°å½±å“
                    - ç»§ç»­è¿½è¸ªå½±å“é“¾è¯·æ±‚ä¸Šä¸‹æ–‡æ”¶é›†ï¼Œæˆ–ç»™å‡ºæœ€ç»ˆç»“è®º

                    ## åˆ†æé‡ç‚¹
                    - åˆ é™¤çš„å‡½æ•°/ç±»æ˜¯å¦è¿˜æœ‰å…¶ä»–åœ°æ–¹åœ¨ä½¿ç”¨
                    - ä¿®æ”¹çš„æ¥å£æ˜¯å¦ä¼šå½±å“è°ƒç”¨æ–¹
                    - æ–°å¢çš„ä»£ç é€»è¾‘æ˜¯å¦æ­£ç¡®
                    - å®Œæ•´çš„å½±å“è·¯å¾„ï¼šä¿®æ”¹A â†’ å½±å“B â†’ å¯¼è‡´Cå¤±æ•ˆ

                    ## è¾“å‡ºæ ¼å¼ï¼ˆçº¯JSONï¼‰

                    ### è¯·æ±‚æ›´å¤šä¸Šä¸‹æ–‡
                    å½“ä½ å‘ç°æŸä¸ªè¢«åˆ é™¤/ä¿®æ”¹çš„å‡½æ•°/ç±»/å˜é‡ï¼Œéœ€è¦çŸ¥é“è°åœ¨ä½¿ç”¨å®ƒæ—¶ï¼š
                    ```json
                    {
                    "action": "request_context",
                    "params": {
                        "search_items": [
                        {
                            "name": "å‡½æ•°åæˆ–ç±»å",
                            "type": "function|class|variable",
                            "reason": "éœ€è¦äº†è§£ä½¿ç”¨æƒ…å†µçš„åŸå› "
                        }
                        ],
                        "analysis_note": "å½“å‰åˆ†æåˆ°çš„æƒ…å†µè¯´æ˜"
                    }
                    }
                    ```

                    ### ç»™å‡ºæœ€ç»ˆç»“è®º
                    å½“ä½ å·²ç»å®Œæˆåˆ†æï¼Œæœ‰æ˜ç¡®ç»“è®ºæ—¶ï¼š
                    ```json
                    {
                    "action": "conclusion",
                    "result": {
                        "has_critical_issues": true|false,
                        "critical_issues": [
                        "å…·ä½“çš„ç¡®å®šæ€§é—®é¢˜æè¿°"
                        ],
                        "impact_chains": [
                        "å½±å“é“¾ï¼šA â†’ B â†’ C"
                        ],
                        "affected_features": ["å—å½±å“çš„åŠŸèƒ½æ¨¡å—"],
                        "summary": "æ€»ä½“åˆ†æç»“è®º"
                    }
                    }
                    ```

                    ## é‡è¦åŸåˆ™
                    1. åŸºäºå®é™…ä»£ç ç»™å‡ºç¡®å®šæ€§ç»“è®º
                    2. å¦‚æœdiffä¸­åˆ é™¤/ä¿®æ”¹äº†æŸä¸ªå®šä¹‰ï¼Œéœ€è¦çŸ¥é“è°åœ¨ä½¿ç”¨å®ƒ
                    3. è¿½è¸ªå®Œæ•´çš„å½±å“é“¾
                    4. å®æ„¿å¤šæœç´¢ï¼Œä¸è¦é—æ¼
                    5. åªè¾“å‡ºJSONï¼Œä¸è¦é¢å¤–è§£é‡Š"""
    
    # æ„å»ºåˆ†ææç¤º
    if iteration_count == 0:
        # é¦–è½®åˆ†æ - ç›´æ¥åˆ†ædiff + ASTç»“æ„
        initial_prompt = f"""
## é¦–è½®åˆ†æ - ä»£ç Diffåˆ†æ + è¯­æ³•æ ‘ç»“æ„

**ä¿®æ”¹è§„æ¨¡**: {pr_size.upper()}
**ä¿®æ”¹æ–‡ä»¶**: {len(changed_files)} ä¸ª

{ast_context}

**Diffä»£ç å—**:
```diff
{pr_diff}
```

**åˆ†æä»»åŠ¡**:
1. **ç»“åˆè¯­æ³•æ ‘ç†è§£ä»£ç ç»“æ„** - ä¸Šé¢çš„è¯­æ³•æ ‘å±•ç¤ºäº†å®Œæ•´çš„å‡½æ•°ã€ç±»ã€æ–¹æ³•å®šä¹‰
2. ä»”ç»†é˜…è¯»diffï¼Œç†è§£å…·ä½“ä¿®æ”¹å†…å®¹
3. è¯†åˆ«åˆ é™¤çš„å‡½æ•°/ç±»/å˜é‡ï¼ˆç”¨-æ ‡è®°çš„è¡Œï¼Œå¯¹ç…§è¯­æ³•æ ‘ç¡®è®¤ï¼‰
4. è¯†åˆ«ä¿®æ”¹çš„å‡½æ•°/ç±»ï¼ˆæ—¢æœ‰-åˆæœ‰+çš„éƒ¨åˆ†ï¼ŒæŸ¥çœ‹è¯­æ³•æ ‘ä¸­çš„å‚æ•°å’Œç±»å‹ï¼‰
5. åˆ¤æ–­è¿™äº›ä¿®æ”¹æ˜¯å¦ä¼šå½±å“å…¶ä»–ä»£ç 

**æ³¨æ„**:
- è¯­æ³•æ ‘æä¾›äº†ç²¾ç¡®çš„ä»£ç ç»“æ„ï¼ŒåŒ…æ‹¬å‡½æ•°å‚æ•°ã€è¿”å›ç±»å‹ã€æ–‡æ¡£å­—ç¬¦ä¸²
- é‡ç‚¹å…³æ³¨åˆ é™¤æ“ä½œï¼ˆ-å¼€å¤´çš„è¡Œï¼‰
- å¦‚æœåˆ é™¤äº†å‡½æ•°/ç±»å®šä¹‰ï¼Œéœ€è¦çŸ¥é“æ˜¯å¦æœ‰å…¶ä»–åœ°æ–¹åœ¨è°ƒç”¨
- ç»™å‡ºåŸºäºä»£ç çš„ç¡®å®šæ€§åˆ¤æ–­
"""
    else:
        # åç»­è¿­ä»£ - åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
        code_snippets_text = ""
        if context_response and 'dependencies' in context_response:
            code_snippets_text = "\n## æ”¶é›†åˆ°çš„ä¸Šä¸‹æ–‡ä»£ç \n\n"
            for item_name, dep_info in context_response['dependencies'].items():
                usage_count = dep_info.get('usage_count', 0)
                code_snippets_text += f"### {item_name} çš„ä½¿ç”¨æƒ…å†µï¼ˆå…±{usage_count}å¤„ï¼‰:\n\n"
                
                snippets = dep_info.get('code_snippets', [])
                if snippets:
                    for snippet in snippets[:]:
                        code_snippets_text += f"**æ–‡ä»¶**: {snippet['file']}\n"
                        code_snippets_text += f"**å‡½æ•°**: {snippet['function']}\n"
                        code_snippets_text += f"**è¡Œå·**: {snippet['line']}\n"
                        code_snippets_text += f"```\n{snippet['context']}\n```\n\n"
                else:
                    code_snippets_text += f"  æœªæ‰¾åˆ°ä½¿ç”¨è¯¥é¡¹çš„ä»£ç \n\n"
        
        # æ„å»ºå½±å“é“¾æ‘˜è¦
        chain_summary = ""
        if impact_chain:
            chain_summary = "\n## å½“å‰å½±å“é“¾è¿½è¸ª\n\n"
            for entry in impact_chain:
                chain_summary += f"**è¿­ä»£{entry['iteration']}**: {entry.get('analysis_note', 'åˆ†æä¸­...')}\n"
        
        initial_prompt = f"""
## ç¬¬ {iteration_count + 1} è½®è¿­ä»£åˆ†æ

{chain_summary}

{code_snippets_text}

**Diffä»£ç ï¼ˆå›é¡¾ï¼‰**:
```diff
{pr_diff}
```

**åˆ†æä»»åŠ¡**:
1. ç»“åˆä¸Šé¢æ”¶é›†åˆ°çš„ä»£ç ä¸Šä¸‹æ–‡
2. åˆ¤æ–­diffä¸­çš„ä¿®æ”¹æ˜¯å¦ä¼šå½±å“è¿™äº›è°ƒç”¨ç‚¹
3. å¦‚æœè¿™äº›è°ƒç”¨ç‚¹è¿˜å¯èƒ½å½±å“å…¶å®ƒå‡½æ•°/ç±»çš„ä½¿ç”¨å°±ç»§ç»­æ”¶é›†ä¸Šä¸‹æ–‡
3. å¦‚æœè¿˜éœ€è¦äº†è§£æ›´å¤šå‡½æ•°/ç±»çš„ä½¿ç”¨æƒ…å†µï¼Œç»§ç»­è¯·æ±‚
4. å¦‚æœå·²ç»æ˜ç¡®å½±å“èŒƒå›´ï¼Œç»™å‡ºæœ€ç»ˆç»“è®º

**è¦æ±‚**:
- åŸºäºå®é™…ä»£ç ç»™å‡ºæ˜ç¡®ç»“è®º
- è¯´æ˜å®Œæ•´çš„å½±å“è·¯å¾„
- é¿å…ä½¿ç”¨"å¯èƒ½"ç­‰æ¨¡ç³Šè¯æ±‡
"""
    
    conversation = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=initial_prompt)
    ]
    
    # è°ƒç”¨LLM
    import hashlib
    call_signature = hashlib.md5(f"{iteration_count}_{len(pr_diff)}".encode()).hexdigest()[:8]
    print(f"[ä»£ç åˆ†æ] ğŸ¤– è°ƒç”¨LLM (è¿­ä»£ {iteration_count + 1}, ID:{call_signature})...")
    
    def validate_schema(data: dict) -> bool:
        if "action" not in data:
            return False
        action = data.get("action")
        if action == "request_context":
            return "params" in data and "search_items" in data["params"]
        elif action == "conclusion":
            return "result" in data
        return False
    
    # ä»é…ç½®è¯»å–LLMé‡è¯•å’Œè¶…æ—¶å‚æ•°
    llm_retry_config = CONFIG.get('llm', {}).get('retry', {})
    llm_timeout_config = CONFIG.get('llm', {}).get('timeout', {})
    max_retries = llm_retry_config.get('code_analyzer', 3)
    timeout = llm_timeout_config.get('code_analyzer', 3600)
    
    result = await parser.parse_json_with_retry(
        conversation=conversation,
        expected_schema={"action": str},
        custom_validator=validate_schema,
        max_retries=max_retries,
        parser_name="code_analyzer_diff",
        timeout=timeout
    )
    
    if not result:
        print(f"[ä»£ç åˆ†æ] âŒ LLMè§£æå¤±è´¥")
        return {
            "analysis_conclusion": {
                "has_critical_issues": False,
                "critical_issues": [],
                "potential_risks": ["LLMå“åº”è§£æå¤±è´¥"],
                "summary": f"ç¬¬{iteration_count + 1}è½®è¿­ä»£è§£æå¤±è´¥"
            },
            "current_stage": "analysis_complete",
            "iteration_count": iteration_count + 1
        }
    
    action = result.get("action")
    
    if action == "conclusion":
        # åˆ†æå®Œæˆ
        conclusion = result.get("result", {})
        print(f"[ä»£ç åˆ†æ] âœ… åˆ†æå®Œæˆ (å…±{iteration_count + 1}è½®)")
        print(f"[ä»£ç åˆ†æ]   - æ˜¯å¦æœ‰é—®é¢˜: {conclusion.get('has_critical_issues', False)}")
        print("="*60 + "\n")
        
        # æ·»åŠ è¿­ä»£ä¿¡æ¯
        conclusion["iteration_info"] = {
            "total_iterations": iteration_count + 1,
            "impact_chain_depth": len(impact_chain)
        }
        
        return {
            "analysis_conclusion": conclusion,
            "current_stage": "analysis_complete",
            "iteration_count": iteration_count + 1,
            "impact_chain": impact_chain,
            "ast_cache": updated_ast_cache
        }
    
    elif action == "request_context":
        # éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡
        params = result.get("params", {})
        search_items = params.get("search_items", [])
        analysis_note = params.get("analysis_note", "")
        
        if not search_items:
            # æ²¡æœ‰æœç´¢é¡¹ï¼Œå¼ºåˆ¶ç»“è®º
            print(f"[ä»£ç åˆ†æ] âš ï¸ æœªæŒ‡å®šæœç´¢é¡¹ï¼Œå¼ºåˆ¶å®Œæˆ")
            return {
                "analysis_conclusion": {
                    "has_critical_issues": False,
                    "critical_issues": [],
                    "potential_risks": ["æœªæŒ‡å®šéœ€è¦æœç´¢çš„é¡¹"],
                    "summary": f"å®Œæˆ{iteration_count + 1}è½®è¿­ä»£"
                },
                "current_stage": "analysis_complete",
                "iteration_count": iteration_count + 1
            }
        
        # æå–æœç´¢é¡¹åç§°
        search_names = [item.get("name", "") for item in search_items if item.get("name")]
        
        if not search_names:
            print(f"[ä»£ç åˆ†æ] âš ï¸ æœç´¢é¡¹ç¼ºå°‘åç§°ï¼Œå¼ºåˆ¶å®Œæˆ")
            return {
                "analysis_conclusion": {
                    "has_critical_issues": False,
                    "critical_issues": [],
                    "potential_risks": ["æœç´¢é¡¹æ ¼å¼é”™è¯¯"],
                    "summary": f"å®Œæˆ{iteration_count + 1}è½®è¿­ä»£"
                },
                "current_stage": "analysis_complete",
                "iteration_count": iteration_count + 1
            }
        
        print(f"[ä»£ç åˆ†æ] ğŸ“‹ è¯·æ±‚æœç´¢: {', '.join(search_names[:])}...")
        print(f"[ä»£ç åˆ†æ] ğŸ’¡ åˆ†æè¯´æ˜: {analysis_note}")
        print(f"[ä»£ç åˆ†æ] ğŸ”„ è½¬äº¤ä¸Šä¸‹æ–‡æ”¶é›†æ™ºèƒ½ä½“...")
        print("="*60 + "\n")
        
        # è®°å½•å½±å“é“¾
        new_chain_entry = {
            "iteration": iteration_count + 1,
            "search_items": search_items,
            "analysis_note": analysis_note
        }
        updated_chain = impact_chain + [new_chain_entry]
        
        return {
            "context_request": {
                "search_items": search_items,
                "analysis_note": analysis_note
            },
            "current_stage": "context_collection",
            "iteration_count": iteration_count + 1,
            "impact_chain": updated_chain
        }
    
    else:
        print(f"[ä»£ç åˆ†æ] âš ï¸ æœªçŸ¥action: {action}")
        return {
            "analysis_conclusion": {
                "has_critical_issues": False,
                "critical_issues": [],
                "potential_risks": [f"æœªçŸ¥action: {action}"],
                "summary": "åˆ†æå¼‚å¸¸"
            },
            "current_stage": "analysis_complete",
            "iteration_count": iteration_count + 1
        }


def _extract_ast_context(changed_files: List[str], repo_path: str, pr_size: str, ast_cache: Dict[str, List]) -> tuple:
    """
    æå–å˜æ›´æ–‡ä»¶çš„ASTè¯­æ³•æ ‘ç»“æ„å¹¶ç¼“å­˜
    ä¸ºLLMæä¾›ç²¾ç¡®çš„ä»£ç ç»“æ„ä¿¡æ¯
    
    Returns:
        (ast_context_str, updated_ast_cache)
    """
    if not AST_AVAILABLE:
        return "\n**æ³¨**: ASTè§£æå™¨æœªå®‰è£…ï¼Œä½¿ç”¨åŸºç¡€åˆ†ææ¨¡å¼\n", ast_cache
    
    parser = ASTParser()
    ast_info = []
    updated_cache = dict(ast_cache)
    
    print(f"[ASTè§£æ] ğŸŒ³ é¦–è½®è§£æ {len(changed_files)} ä¸ªå˜æ›´æ–‡ä»¶...")
    
    for file_path in changed_files[:]:
        full_path = os.path.join(repo_path, file_path)
        
        if not os.path.exists(full_path):
            continue
        
        try:
            # è§£æAST
            nodes = parser.parse_file(full_path)
            
            if nodes:
                # å­˜å…¥ç¼“å­˜
                updated_cache[file_path] = nodes
                
                # ç”ŸæˆLLMå‹å¥½çš„è¯­æ³•æ ‘æ‘˜è¦
                ast_summary = parser.generate_llm_context(nodes, include_docstring=True)
                
                ast_info.append(f"""
                                    ### ğŸ“„ {file_path} - ä»£ç ç»“æ„ï¼ˆè¯­æ³•æ ‘ï¼‰

                                    {ast_summary}
                                    """)
                
                print(f"[ASTè§£æ]   âœ“ {file_path}: {len(nodes)} ä¸ªå®šä¹‰ â†’ å·²ç¼“å­˜")
        
        except Exception as e:
            print(f"[ASTè§£æ]   âš ï¸ {file_path}: è§£æå¤±è´¥ - {e}")
            continue
    
    if ast_info:
        header = f"""
                    ## ğŸŒ³ ä»£ç è¯­æ³•æ ‘ç»“æ„ï¼ˆASTè§£æï¼‰

                    ä»¥ä¸‹æ˜¯å˜æ›´æ–‡ä»¶çš„ç²¾ç¡®ä»£ç ç»“æ„ï¼ŒåŒ…å«æ‰€æœ‰å‡½æ•°ã€ç±»ã€æ–¹æ³•çš„å®šä¹‰ï¼š
                    - **å‡½æ•°å‚æ•°**: æ˜ç¡®åˆ—å‡ºï¼Œæ–¹ä¾¿ç†è§£æ¥å£
                    - **è¿”å›ç±»å‹**: æ˜¾ç¤ºç±»å‹ä¿¡æ¯ï¼ˆå¦‚æœ‰ï¼‰
                    - **æ–‡æ¡£å­—ç¬¦ä¸²**: ç†è§£å‡½æ•°ç”¨é€”
                    - **æ‰€å±å…³ç³»**: æ–¹æ³•å±äºå“ªä¸ªç±»

                    è¿™äº›ä¿¡æ¯å¸®åŠ©ä½ å‡†ç¡®ç†è§£ä»£ç ä¿®æ”¹çš„å½±å“èŒƒå›´ã€‚

                    {''.join(ast_info)}

                    ---
                    """
        print(f"[ASTè§£æ] âœ… é¦–è½®è¯­æ³•æ ‘è§£æå®Œæˆï¼Œå…± {len(ast_info)} ä¸ªæ–‡ä»¶å·²ç¼“å­˜")
        return header, updated_cache
    else:
        return "\n**æ³¨**: æ— æ³•è§£æè¯­æ³•æ ‘ï¼Œå°†ä»…åŸºäºdiffæ–‡æœ¬åˆ†æ\n", updated_cache


def _generate_ast_context_from_cache(changed_files: List[str], ast_cache: Dict[str, List], pr_size: str) -> str:
    """
    ä»ç¼“å­˜ä¸­ç”ŸæˆASTä¸Šä¸‹æ–‡ï¼ˆåç»­è½®ä½¿ç”¨ï¼‰
    
    Args:
        changed_files: å˜æ›´æ–‡ä»¶åˆ—è¡¨
        ast_cache: ASTç¼“å­˜
        pr_size: PRè§„æ¨¡
        
    Returns:
        ASTä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    """
    if not AST_AVAILABLE or not ast_cache:
        return "\n**æ³¨**: ASTç¼“å­˜ä¸ºç©º\n"
    
    # æ ¹æ®PRè§„æ¨¡é™åˆ¶æ–‡ä»¶æ•°
    size_limits = {
        'small': 10,
        'medium': 6,
        'large': 4,
        'xlarge': 2
    }
    max_files = size_limits.get(pr_size, 3)
    
    parser = ASTParser()
    ast_info = []
    cached_count = 0
    
    print(f"[ASTè§£æ] ğŸ“¦ ä»ç¼“å­˜è¯»å–è¯­æ³•æ ‘...")
    
    for file_path in changed_files[:max_files]:
        if file_path in ast_cache:
            nodes = ast_cache[file_path]
            
            ast_summary = parser.generate_llm_context(nodes, include_docstring=True)
            
            ast_info.append(f"""
                            ### ğŸ“„ {file_path} - ä»£ç ç»“æ„ï¼ˆè¯­æ³•æ ‘ï¼‰

                            {ast_summary}
                            """)
            cached_count += 1
            print(f"[ASTè§£æ]   âœ“ {file_path}: ä»ç¼“å­˜è¯»å–")
    
    if ast_info:
        header = f"""
                    ## ğŸŒ³ ä»£ç è¯­æ³•æ ‘ç»“æ„ï¼ˆä»ç¼“å­˜è¯»å–ï¼‰

                    ä»¥ä¸‹æ˜¯å˜æ›´æ–‡ä»¶çš„ç²¾ç¡®ä»£ç ç»“æ„ï¼ŒåŒ…å«æ‰€æœ‰å‡½æ•°ã€ç±»ã€æ–¹æ³•çš„å®šä¹‰ï¼š

                    {''.join(ast_info)}

                    ---
                    """
        print(f"[ASTè§£æ] âœ… ä»ç¼“å­˜è¯»å– {cached_count} ä¸ªæ–‡ä»¶çš„è¯­æ³•æ ‘")
        return header
    else:
        return "\n**æ³¨**: ç¼“å­˜ä¸­æ— ç›¸å…³ASTä¿¡æ¯\n"
