"""
ä¸Šä¸‹æ–‡æ”¶é›†æ™ºèƒ½ä½“èŠ‚ç‚¹
æ ¹æ®ä»£ç åˆ†ææ™ºèƒ½ä½“çš„éœ€æ±‚æ”¶é›†ä»£ç ä¸Šä¸‹æ–‡ä¿¡æ¯
ä½¿ç”¨ Ripgrepæœç´¢ + ASTä»£ç å—æå–
"""

from src.core.state import PRReviewState
from src.utils.config import CONFIG
from src.analyzers.project_analyzer.fast_file_searcher import FastFileSearcher
import os
import json
import re
from typing import Dict, List, Any, Optional

# å¯¼å…¥ASTè§£æå™¨
try:
    from src.analyzers.project_analyzer.ast_parser import ASTParser
    AST_PARSER_AVAILABLE = True
except ImportError:
    AST_PARSER_AVAILABLE = False
    ASTParser = None


async def context_collector_node(state: PRReviewState) -> PRReviewState:
    """ä¸Šä¸‹æ–‡æ”¶é›†æ™ºèƒ½ä½“
    
    èŒè´£ï¼š
    1. æ¥æ”¶ä»£ç åˆ†ææ™ºèƒ½ä½“çš„æœç´¢è¯·æ±‚
    2. åœ¨ä»£ç ä»“åº“ä¸­æœç´¢æŒ‡å®šçš„å‡½æ•°/ç±»/å˜é‡
    3. æå–ä½¿ç”¨è¿™äº›é¡¹çš„ä»£ç ç‰‡æ®µ
    4. è¿”å›ä¸Šä¸‹æ–‡ä¿¡æ¯ç»™ä»£ç åˆ†ææ™ºèƒ½ä½“
    """
    print("\n" + "="*60)
    print("=== ä¸Šä¸‹æ–‡æ”¶é›†æ™ºèƒ½ä½“ ===")
    print("="*60)
    
    # è·å–è¯·æ±‚
    context_request = state.get("context_request")
    pr_size = state.get("pr_size", "medium")
    iteration_count = state.get("iteration_count", 0)
    all_collected_context = state.get("all_collected_context", {})
    
    if not context_request:
        print("[ä¸Šä¸‹æ–‡æ”¶é›†] âš ï¸ æ— æ”¶é›†è¯·æ±‚ï¼Œè·³è¿‡")
        return {"current_stage": "code_analysis"}
    
    search_items = context_request.get('search_items', [])
    analysis_note = context_request.get('analysis_note', '')
    
    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†] ğŸ“Š è¿­ä»£ {iteration_count}")
    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†] ğŸ” æœç´¢é¡¹: {len(search_items)} ä¸ª")
    if analysis_note:
        print(f"[ä¸Šä¸‹æ–‡æ”¶é›†] ğŸ’¡ åˆ†æè¯´æ˜: {analysis_note}")
    
    # åˆå§‹åŒ–å·¥å…·
    repo_path = CONFIG['git_repo']['repo_path']
    file_searcher = FastFileSearcher()
    
    # è·å–ASTç¼“å­˜
    ast_cache = state.get("ast_cache", {})
    
    # æ˜¾ç¤ºåŠŸèƒ½çŠ¶æ€
    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†] ğŸ› ï¸ åŠŸèƒ½çŠ¶æ€:")
    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   - Ripgrepæœç´¢å¼•æ“: âœ“")
    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   - ASTä»£ç å—æå–: {'âœ“' if AST_PARSER_AVAILABLE else 'âœ— (é™çº§ä¸ºå›ºå®šè¡Œæ•°)'}")
    
    # æ ¹æ®è§„æ¨¡è°ƒæ•´å‚æ•°
    size_config = {
        'small': {'max_files_per_item': 30, 'max_matches_per_file': 6},
        'medium': {'max_files_per_item': 20, 'max_matches_per_file': 4},
        'large': {'max_files_per_item': 16, 'max_matches_per_file': 4},
        'xlarge': {'max_files_per_item': 10, 'max_matches_per_file': 2}
    }
    
    config = size_config.get(pr_size, size_config['medium'])
    
    # æ”¶é›†æ¯ä¸ªæœç´¢é¡¹çš„ä½¿ç”¨æƒ…å†µ
    dependencies = {}
    
    # ç¬¬ä¸€é˜¶æ®µï¼šåˆ†ç¦»å·²ç¼“å­˜å’Œéœ€è¦æœç´¢çš„é¡¹
    items_to_search = []
    for item in search_items:
        item_name = item.get('name', '')
        item_type = item.get('type', 'unknown')
        
        if not item_name:
            print(f"[ä¸Šä¸‹æ–‡æ”¶é›†] âš ï¸ è·³è¿‡ç©ºåç§°çš„æœç´¢é¡¹")
            continue
        
        # æ£€æŸ¥æ˜¯å¦å·²æ”¶é›†è¿‡
        if item_name in all_collected_context:
            print(f"[ä¸Šä¸‹æ–‡æ”¶é›†] â„¹ï¸ {item_type}: {item_name} - å·²ç¼“å­˜ï¼Œå¤ç”¨")
            dependencies[item_name] = all_collected_context[item_name]
        else:
            items_to_search.append(item)
    
    # ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ Ripgrep + AST æœç´¢
    updated_ast_cache = ast_cache
    if items_to_search:
        print(f"[ä¸Šä¸‹æ–‡æ”¶é›†] ğŸš€ æ‰¹é‡æœç´¢ {len(items_to_search)} ä¸ªæ–°é¡¹...")
        dependencies, updated_ast_cache = _ripgrep_ast_search(
            items_to_search,
            file_searcher,
            config,
            repo_path,
            dependencies,
            ast_cache
        )
    
    # åˆå¹¶åˆ°ç´¯ç§¯ä¸Šä¸‹æ–‡
    updated_all_context = {**all_collected_context, **dependencies}
    
    print(f"\n[ä¸Šä¸‹æ–‡æ”¶é›†] âœ… æ”¶é›†å®Œæˆ")
    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   - æœ¬è½®æ”¶é›†: {len(dependencies)} ä¸ªé¡¹")
    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   - ç´¯ç§¯æ€»æ•°: {len(updated_all_context)} ä¸ª")
    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   - ASTç¼“å­˜: {len(updated_ast_cache)} ä¸ªæ–‡ä»¶")
    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†] ğŸ”„ è¿”å›ä»£ç åˆ†ææ™ºèƒ½ä½“...")
    print("="*60 + "\n")
    
    return {
        "context_response": {
            "dependencies": dependencies,
            "summary": f"æ”¶é›†äº†{len(dependencies)}ä¸ªé¡¹çš„ä½¿ç”¨ä¿¡æ¯",
            "iteration": iteration_count
        },
        "all_collected_context": updated_all_context,
        "context_request": None,
        "current_stage": "code_analysis",
        "ast_cache": updated_ast_cache
    }


def _build_search_patterns(name: str, item_type: str) -> List[str]:
    """æ ¹æ®ç±»å‹æ„å»ºæœç´¢æ¨¡å¼ - æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€"""
    patterns = []
    
    # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
    escaped_name = re.escape(name)
    
    if item_type == 'function':
        # å‡½æ•°è°ƒç”¨æ¨¡å¼ï¼ˆPython/C++/C#/Javaï¼‰
        patterns.extend([
            rf'\b{escaped_name}\s*\(',  # å‡½æ•°è°ƒç”¨
            rf'from\s+\S+\s+import\s+.*\b{escaped_name}\b',  # Python import
            rf'import\s+.*\b{escaped_name}\b',  # import
        ])
    elif item_type == 'class':
        # ç±»ä½¿ç”¨æ¨¡å¼ï¼ˆPython/C++/C#/Javaï¼‰
        patterns.extend([
            rf'\b{escaped_name}\b',  # é€šç”¨åŒ¹é…ï¼ˆæœ€é‡è¦ï¼‰
            rf'\b{escaped_name}\s*\(',  # å®ä¾‹åŒ–
            rf'\b{escaped_name}\s*\*',  # C++æŒ‡é’ˆ
            rf'\b{escaped_name}\s*&',  # C++å¼•ç”¨
            rf':\s*(?:public|private|protected)?\s*{escaped_name}\b',  # C++ç»§æ‰¿
            rf':\s*{escaped_name}\b',  # ç±»å‹æ³¨è§£/ç»§æ‰¿
            rf'class\s+\w+\s*:\s*.*{escaped_name}',  # C#/Javaç»§æ‰¿
            rf'new\s+{escaped_name}\b',  # newå…³é”®å­—
            rf'from\s+\S+\s+import\s+.*\b{escaped_name}\b',  # import
            rf'isinstance\s*\([^,]+,\s*{escaped_name}\b',  # isinstanceæ£€æŸ¥
        ])
    elif item_type == 'variable':
        # å˜é‡ä½¿ç”¨æ¨¡å¼ - åŒ…æ‹¬å­—ç¬¦ä¸²å­—é¢é‡ï¼ˆç”¨äºæ³¨å†Œè¡¨é”®ç­‰ï¼‰
        patterns.extend([
            rf'\b{escaped_name}\b',  # ç›´æ¥ä½¿ç”¨
            rf'["\'].*{escaped_name}.*["\']',  # å­—ç¬¦ä¸²ä¸­ï¼ˆå¤§å°å†™æ•æ„Ÿï¼‰
            rf'L?".*{escaped_name}.*"',  # C++ wide string
            rf'@".*{escaped_name}.*"',  # C# verbatim string
            rf'from\s+\S+\s+import\s+.*\b{escaped_name}\b',  # import
        ])
        
        # å¯¹äºé…ç½®é¡¹ï¼Œæ·»åŠ ä¸åŒå¤§å°å†™å˜ä½“
        # ä¾‹å¦‚: notifyOnSuccess -> NotifyOnSuccess, NOTIFY_ON_SUCCESS
        if name[0].islower():  # å¦‚æœæ˜¯å°é©¼å³°
            # è½¬æ¢ä¸ºå¤§é©¼å³° (Pascal Case)
            pascal_case = name[0].upper() + name[1:]
            patterns.append(rf'\b{re.escape(pascal_case)}\b')
            patterns.append(rf'["\'].*{re.escape(pascal_case)}.*["\']')
            
            # è½¬æ¢ä¸ºå¤§å†™ä¸‹åˆ’çº¿ (SCREAMING_SNAKE_CASE)
            import re as re_module
            snake_case = re_module.sub(r'([A-Z])', r'_\1', name).upper()
            if snake_case != name.upper():
                patterns.append(rf'\b{re.escape(snake_case)}\b')
    else:
        # é€šç”¨æ¨¡å¼
        patterns.append(rf'\b{escaped_name}\b')
    
    return patterns


def _simplify_matches(matches_dict: Dict[str, List[Dict]]) -> Dict[str, List[str]]:
    """ç®€åŒ–åŒ¹é…ç»“æœï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯"""
    simplified = {}
    for file_path, matches in matches_dict.items():
        simplified[file_path] = [
            f"L{m.get('line_number', '?')}: {str(m.get('line', ''))[:50]}"
            for m in matches[:2]  # æ¯ä¸ªæ–‡ä»¶æœ€å¤š2ä¸ªç¤ºä¾‹
        ]
    return simplified


def _extract_code_context(repo_path: str, matches_dict: Dict[str, List[Dict]], ast_cache: Dict[str, List]) -> tuple:
    """
    æå–ä»£ç ä¸Šä¸‹æ–‡ç‰‡æ®µç”¨äºæ·±åº¦åˆ†æ
    ä¼˜å…ˆä½¿ç”¨ASTç²¾ç¡®æå–å®Œæ•´ä»£ç å—ï¼Œfallbackåˆ°å›ºå®šè¡Œæ•°
    
    Returns:
        (code_snippets, updated_ast_cache)
    """
    code_snippets = []
    updated_cache = dict(ast_cache) 
    
    # åˆå§‹åŒ–ASTè§£æå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    ast_parser = None
    if AST_PARSER_AVAILABLE:
        try:
            ast_parser = ASTParser()
            if ast_parser.available:
                print("[ä¸Šä¸‹æ–‡æ”¶é›†]   ğŸŒ² ä½¿ç”¨ASTç²¾ç¡®æå–ä»£ç å— + ç¼“å­˜")
            else:
                ast_parser = None
        except Exception as e:
            print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   âš ï¸ ASTè§£æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            ast_parser = None
    
    for file_path, matches in list(matches_dict.items())[:3]:  # æœ€å¤š3ä¸ªæ–‡ä»¶
        try:
            full_path = os.path.join(repo_path, file_path)
            if not os.path.exists(full_path):
                continue
                
            with open(full_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æ£€æŸ¥ASTç¼“å­˜æˆ–è§£ææ–°æ–‡ä»¶
            ast_nodes = []
            if ast_parser:
                if file_path in updated_cache:
                    # ä»ç¼“å­˜è¯»å–
                    ast_nodes = updated_cache[file_path]
                    print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   ğŸ“¦ {file_path}: ä»ASTç¼“å­˜è¯»å–")
                else:
                    # è§£ææ–°æ–‡ä»¶å¹¶åŠ å…¥ç¼“å­˜
                    try:
                        ast_nodes = ast_parser.parse_file(full_path)
                        if ast_nodes:
                            updated_cache[file_path] = ast_nodes
                            print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   ğŸŒ² {file_path}: è§£æASTå¹¶ç¼“å­˜ ({len(ast_nodes)} ä¸ªå®šä¹‰)")
                    except Exception as e:
                        print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   âš ï¸ ASTè§£æå¤±è´¥ {file_path}: {e}")
            
            for match in matches[:5]:  # æ¯æ–‡ä»¶æœ€å¤š2ä¸ªåŒ¹é…
                line_num = match.get('line_number', 0)
                if line_num <= 0:
                    continue
                
                # å°è¯•ä½¿ç”¨ASTå®šä½ä»£ç å—
                if ast_nodes:
                    snippet = _extract_ast_code_block(
                        file_path, 
                        line_num, 
                        lines, 
                        ast_nodes, 
                        match
                    )
                    if snippet:
                        code_snippets.append(snippet)
                        continue
                
                # Fallback: å›ºå®šè¡Œæ•°æå–
                snippet = _extract_fixed_lines_context(
                    file_path,
                    line_num,
                    lines,
                    match
                )
                if snippet:
                    code_snippets.append(snippet)
                    
        except Exception as e:
            print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   âš ï¸ æå–ä»£ç ä¸Šä¸‹æ–‡å‡ºé”™: {e}")
            continue
    
    return code_snippets, updated_cache


def _extract_ast_code_block(
    file_path: str,
    line_num: int,
    lines: List[str],
    ast_nodes: List,
    match: Dict
) -> Optional[Dict]:
    """
    ä½¿ç”¨ASTç²¾ç¡®æå–åŒ…å«æŒ‡å®šè¡Œçš„å®Œæ•´ä»£ç å—
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        line_num: åŒ¹é…çš„è¡Œå·
        lines: æ–‡ä»¶æ‰€æœ‰è¡Œ
        ast_nodes: ASTèŠ‚ç‚¹åˆ—è¡¨
        match: åŸå§‹åŒ¹é…ä¿¡æ¯
        
    Returns:
        ä»£ç ç‰‡æ®µå­—å…¸ï¼ŒåŒ…å«å®Œæ•´çš„å‡½æ•°/ç±»å®šä¹‰
    """
    try:
        # æŸ¥æ‰¾åŒ…å«è¯¥è¡Œçš„æœ€å°ASTèŠ‚ç‚¹ï¼ˆå‡½æ•°æˆ–ç±»ï¼‰
        enclosing_node = None
        for node in ast_nodes:
            if node.line_number <= line_num <= node.end_line:
                # æ‰¾åˆ°åŒ…å«è¯¥è¡Œçš„èŠ‚ç‚¹ï¼Œä¼˜å…ˆé€‰æ‹©æœ€å°çš„ï¼ˆæœ€å…·ä½“çš„ï¼‰
                if enclosing_node is None or (
                    node.end_line - node.line_number < 
                    enclosing_node.end_line - enclosing_node.line_number
                ):
                    enclosing_node = node
        
        if not enclosing_node:
            # æ²¡æ‰¾åˆ°åŒ…å«è¯¥è¡Œçš„èŠ‚ç‚¹ï¼Œè¿”å›Noneä»¥fallback
            return None
        
        # æå–å®Œæ•´ä»£ç å—
        start_line = enclosing_node.line_number - 1  # è½¬ä¸º0-basedç´¢å¼•
        end_line = enclosing_node.end_line  # end_lineå·²ç»æ˜¯åŒ…å«çš„
        
        # ç¡®ä¿ä¸è¶Šç•Œ
        start_line = max(0, start_line)
        end_line = min(len(lines), end_line)
        
        code_block = ''.join(lines[start_line:end_line])
        
        # è®¡ç®—ä»£ç å—å¤§å°ï¼ˆé™åˆ¶è¿‡å¤§çš„å—ï¼‰
        block_lines = end_line - start_line
        if block_lines > 300:
            # å¦‚æœä»£ç å—å¤ªå¤§ï¼Œåªæå–å…³é”®éƒ¨åˆ†
            # å–åŒ¹é…è¡Œå‰åå„50è¡Œï¼Œä½†ä¸è¶…è¿‡ä»£ç å—èŒƒå›´
            context_start = max(start_line, line_num - 51)
            context_end = min(end_line, line_num + 50)
            code_block = ''.join(lines[context_start:context_end])
            block_info = f"[ä»£ç å—è¿‡å¤§ï¼Œæ˜¾ç¤ºéƒ¨åˆ†: L{context_start+1}-L{context_end}]"
        else:
            block_info = f"[å®Œæ•´{enclosing_node.type}å®šä¹‰: L{start_line+1}-L{end_line}]"
        
        # æ„å»ºè¿”å›ä¿¡æ¯
        return {
            "file": file_path,
            "line": line_num,
            "function": enclosing_node.name,
            "type": enclosing_node.type,
            "start_line": start_line + 1,
            "end_line": end_line,
            "context": code_block,
            "matched_line": lines[line_num - 1] if line_num <= len(lines) else "",
            "extraction_method": "AST",
            "block_info": block_info,
            "docstring": enclosing_node.docstring if hasattr(enclosing_node, 'docstring') else None,
            "params": enclosing_node.params if hasattr(enclosing_node, 'params') else None
        }
        
    except Exception as e:
        print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   âš ï¸ ASTä»£ç å—æå–å¤±è´¥: {e}")
        return None


def _extract_fixed_lines_context(
    file_path: str,
    line_num: int,
    lines: List[str],
    match: Dict
) -> Optional[Dict]:
    """
    å›ºå®šè¡Œæ•°æå–ï¼ˆFallbackæ–¹æ³•ï¼‰
    æå–åŒ¹é…è¡Œå‰åå„5è¡Œ
    """
    try:
        # æå–ä¸Šä¸‹æ–‡ï¼šå‰åå„5è¡Œ
        start = max(0, line_num - 11)
        end = min(len(lines), line_num + 10)
        context_lines = lines[start:end]
        
        # æŸ¥æ‰¾æ‰€å±å‡½æ•°/ç±»ï¼ˆä½¿ç”¨æ­£åˆ™ï¼‰
        function_name = _find_enclosing_function(lines, line_num)
        
        return {
            "file": file_path,
            "line": line_num,
            "function": function_name,
            "type": "unknown",
            "start_line": start + 1,
            "end_line": end,
            "context": ''.join(context_lines),
            "matched_line": lines[line_num - 1] if line_num <= len(lines) else "",
            "extraction_method": "fixed_lines",
            "block_info": f"[å‰å5è¡Œä¸Šä¸‹æ–‡: L{start+1}-L{end}]"
        }
        
    except Exception as e:
        print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   âš ï¸ å›ºå®šè¡Œæ•°æå–å¤±è´¥: {e}")
        return None


def _find_enclosing_function(lines: List[str], line_num: int) -> str:
    """æŸ¥æ‰¾åŒ…å«æŒ‡å®šè¡Œçš„å‡½æ•°/ç±»å"""
    import re
    
    # å‘ä¸ŠæŸ¥æ‰¾å‡½æ•°æˆ–ç±»å®šä¹‰
    for i in range(line_num - 1, max(0, line_num - 50), -1):
        line = lines[i]
        
        # åŒ¹é…å‡½æ•°å®šä¹‰
        func_match = re.match(r'\s*(async\s+)?def\s+(\w+)', line)
        if func_match:
            return func_match.group(2)
        
        # åŒ¹é…ç±»å®šä¹‰
        class_match = re.match(r'\s*class\s+(\w+)', line)
        if class_match:
            return class_match.group(1)
    
    return "æœªçŸ¥å‡½æ•°"


def _ripgrep_ast_search(
    items_to_search: List[Dict],
    file_searcher: FastFileSearcher,
    config: Dict,
    repo_path: str,
    dependencies: Dict,
    ast_cache: Dict[str, List]
) -> tuple:
    """
    ä½¿ç”¨ Ripgrep + AST è¿›è¡Œä»£ç æœç´¢å’Œä¸Šä¸‹æ–‡æå–
    
    æµç¨‹ï¼š
    1. Ripgrepå¿«é€Ÿå®šä½ç¬¦å·ä½¿ç”¨ä½ç½®
    2. ASTç²¾ç¡®æå–å®Œæ•´ä»£ç å—å¹¶ç¼“å­˜
    3. è¿”å›ç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    
    Returns:
        (dependencies, updated_ast_cache)
    """
    print("[ä¸Šä¸‹æ–‡æ”¶é›†] ğŸ” Ripgrepæœç´¢ + ASTä»£ç å—æå–")
    updated_ast_cache = ast_cache
    
    # æ„å»ºæ‰¹é‡æœç´¢æ¨¡å¼
    batch_patterns = []
    item_pattern_map = {}
    file_pattern = "*.py,*.cpp,*.h,*.hpp,*.c,*.cs,*.java,*.js,*.ts"
    
    for item in items_to_search:
        item_name = item.get('name', '')
        item_type = item.get('type', 'unknown')
        
        print(f"[ä¸Šä¸‹æ–‡æ”¶é›†] ğŸ” {item_type}: {item_name}")
        if item.get('reason'):
            print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   åŸå› : {item['reason']}")
        
        patterns = _build_search_patterns(item_name, item_type)
        
        for pattern in patterns:
            pattern_key = f"{pattern}|{file_pattern}"
            batch_patterns.append((pattern, file_pattern))
            if pattern_key not in item_pattern_map:
                item_pattern_map[pattern_key] = []
            item_pattern_map[pattern_key].append(item)
    
    # æ‰§è¡Œæ‰¹é‡æœç´¢
    batch_results = file_searcher.batch_search(repo_path, batch_patterns)
    
    # æ•´ç†æœç´¢ç»“æœ
    for item in items_to_search:
        item_name = item.get('name', '')
        item_type = item.get('type', 'unknown')
        item_reason = item.get('reason', '')
        
        # åˆå¹¶è¯¥é¡¹ç›¸å…³çš„æ‰€æœ‰æœç´¢ç»“æœ
        all_results = {}
        patterns = _build_search_patterns(item_name, item_type)
        
        for pattern in patterns:
            pattern_key = f"{pattern}|{file_pattern}"
            if pattern_key in batch_results:
                results = batch_results[pattern_key]
                for file_path, matches in results.items():
                    if file_path in all_results:
                        all_results[file_path].extend(matches)
                    else:
                        all_results[file_path] = matches
        
        # å»é‡å’Œé™åˆ¶ç»“æœ
        limited_results = {}
        for file_path, matches in list(all_results.items())[:config['max_files_per_item']]:
            unique_matches = []
            seen_lines = set()
            for match in matches:
                line_num = match.get('line_number')
                if line_num and line_num not in seen_lines:
                    seen_lines.add(line_num)
                    unique_matches.append(match)
            
            limited_results[file_path] = unique_matches[:config['max_matches_per_file']]
        
        usage_count = sum(len(matches) for matches in limited_results.values())
        
        # æå–ä»£ç ä¸Šä¸‹æ–‡ç‰‡æ®µï¼ˆä½¿ç”¨AST + ç¼“å­˜ï¼‰
        code_snippets, updated_ast_cache = _extract_code_context(repo_path, limited_results, updated_ast_cache)
        
        dependencies[item_name] = {
            "type": item_type,
            "reason": item_reason,
            "used_in_files": list(limited_results.keys()),
            "usage_count": usage_count,
            "usage_details": _simplify_matches(limited_results),
            "code_snippets": code_snippets,
            "search_status": "æœªæ‰¾åˆ°ä½¿ç”¨" if usage_count == 0 else f"æ‰¾åˆ°{usage_count}å¤„ä½¿ç”¨"
        }
        
        print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   âœ“ {dependencies[item_name]['search_status']}")
        if code_snippets:
            ast_count = sum(1 for s in code_snippets if s.get('extraction_method') == 'AST')
            if ast_count > 0:
                print(f"[ä¸Šä¸‹æ–‡æ”¶é›†]   ğŸŒ² ASTæå–: {ast_count}/{len(code_snippets)} ä¸ªä»£ç å—")
    
    return dependencies, updated_ast_cache
